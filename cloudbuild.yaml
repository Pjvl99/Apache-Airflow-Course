# cloudbuild.yaml
options:
  logging: CLOUD_LOGGING_ONLY

substitutions:
  # ---- Composer target ----
  _COMPOSER_ENV: curso-airflow
  _LOCATION: us-east1

  # ---- Airflow Variables to set in the environment ----
  _VAR_REPOSITORY: transformacion
  _VAR_PROJECT: melodic-subject-467218-g1
  _VAR_LOCATION: us-east1

  # IMPORTANTE: para Composer 2 normalmente el prefijo incluye /dags
  # Ejemplo: gs://<bucket-id>/dags
  _DAGS_PREFIX_: gs://us-east1-curso-airflow-b7cd9d90-bucket/dags

steps:

  - name: python
    id: "Instalar paquetes de python"
    entrypoint: pip
    args: ["install", "-r", "requirements.txt", "--user"]

  # - name: gcr.io/google.com/cloudsdktool/cloud-sdk:slim
  #   id: "Activando secretos"
  #   entrypoint: bash
  #   args:
  #     - -c
  #     - |
  #       gcloud composer environments update "${_COMPOSER_ENV}" \
  #         --location "${_VAR_LOCATION}" \
  #         --update-airflow-configs=secrets-backend=airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend,secrets-backend_kwargs='{"project_id":"${_VAR_PROJECT}"}'

  # 1) Sync local /dags folder to the Composer DAGs bucket (deletes removed files)
  - name: gcr.io/google.com/cloudsdktool/cloud-sdk:slim
    id: "Sync DAGs to Composer"
    entrypoint: bash
    args:
      - -c
      - |
        set -euo pipefail
        if [[ ! -d "./dags" ]]; then
          echo "ERROR: ./dags directory not found in repo root." >&2
          exit 1
        fi
        # Sustitución de Cloud Build => usa UN solo $
        gsutil -m rsync -r -d ./dags "${_DAGS_PREFIX_}"

  # 2) Update Composer environment PyPI packages from requirements.txt
  # - name: gcr.io/google.com/cloudsdktool/cloud-sdk:slim
  #   id: "Update PyPI packages from requirements.txt"
  #   entrypoint: bash
  #   args:
  #     - -c
  #     - |
  #       set -euo pipefail
  #       if [[ ! -f "requirements.txt" ]]; then
  #         echo "requirements.txt not found in repo root; skipping." >&2
  #         exit 1
  #       fi
  #       # Sustituciones de Cloud Build: un solo $
  #       gcloud composer environments update "${_COMPOSER_ENV}" \
  #         --location="${_LOCATION}" \
  #         --update-pypi-packages-from-file="requirements.txt"

  # # 3) Crear/actualizar conexión 'slack' como secreto de Airflow en Secret Manager
  # - name: gcr.io/google.com/cloudsdktool/cloud-sdk:slim
  #   id: "Sync Airflow connection 'slack' from Secret Manager"
  #   entrypoint: bash
  #   args:
  #     - -c
  #     - |
  #       set -euo pipefail

  #       # Variables de SHELL => normales aquí (no son sustituciones de Cloud Build)
  #       SLACK_TOKEN="$(gcloud secrets versions access latest --secret=slack)"

  #       # Para que Cloud Build NO intente sustituir, usa $$ al referenciarlas en el YAML
  #       CONN_JSON=$(printf '{"conn_type":"generic","password":"%s"}' "$$SLACK_TOKEN")

  #       # Idempotente: si existe el secreto, añadimos una nueva versión; si no, lo creamos.
  #       printf '%s' "$$CONN_JSON" | gcloud secrets create "airflow-connections-slack" --data-file=- --replication-policy=automatic
  #       fi

  # 4) Set Airflow Variables (idempotent)
  - name: gcr.io/google.com/cloudsdktool/cloud-sdk:slim
    id: "Sync Airflow Variables (Secret Manager)"
    entrypoint: bash
    args:
      - -c
      - |
        set -euo pipefail
        put_secret () {
          local name="$1" ; local value="$2"
          if gcloud secrets describe "$name" >/dev/null 2>&1; then
            printf '%s' "$value" | gcloud secrets versions add "$name" --data-file=-
          else
            printf '%s' "$value" | gcloud secrets create "$name" --data-file=- --replication-policy=automatic
          fi
        }
        # Graba tus variables como secretos de Secret Manager:
        put_secret "airflow-variables-bucket"     "${_DAGS_PREFIX_}"
        put_secret "airflow-variables-repository" "${_VAR_REPOSITORY}"
        put_secret "airflow-variables-project"    "${_VAR_PROJECT}"
        put_secret "airflow-variables-location"   "${_VAR_LOCATION}"
